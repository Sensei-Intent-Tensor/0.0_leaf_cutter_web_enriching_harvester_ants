# 01_ant_anatomy

> **The Building Blocks of Every Scraper**

In the Leaf Cutter framework, every scraper is an "Ant" - a specialized worker designed to harvest specific data from the web. This section defines the anatomy of these ants: their structure, patterns, outputs, and error handling.

---

## ğŸœ What is an Ant?

An **Ant** is a self-contained scraping unit that:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ANT ANATOMY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   INPUT                                                         â”‚
â”‚   â”œâ”€â”€ URL(s) to scrape                                          â”‚
â”‚   â”œâ”€â”€ Configuration (headers, proxies, delays)                  â”‚
â”‚   â””â”€â”€ Extraction rules (selectors, patterns)                    â”‚
â”‚                                                                 â”‚
â”‚   PROCESSING                                                    â”‚
â”‚   â”œâ”€â”€ Fetch content (HTTP or browser)                           â”‚
â”‚   â”œâ”€â”€ Parse HTML/JSON                                           â”‚
â”‚   â”œâ”€â”€ Extract structured data                                   â”‚
â”‚   â””â”€â”€ Handle errors and retries                                 â”‚
â”‚                                                                 â”‚
â”‚   OUTPUT                                                        â”‚
â”‚   â”œâ”€â”€ Structured data (dict, dataclass, Pydantic model)         â”‚
â”‚   â”œâ”€â”€ Metadata (timestamps, source URL, status)                 â”‚
â”‚   â””â”€â”€ Logs and metrics                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Section Contents

| File | Description |
|------|-------------|
| [01_base_ant.py](01_base_ant.py) | The BaseAnt class - foundation for all scrapers |
| [02_ant_patterns.md](02_ant_patterns.md) | Common scraping patterns and when to use them |
| [03_output_formats.md](03_output_formats.md) | Structuring and validating scraped data |
| [04_error_handling.md](04_error_handling.md) | Robust error handling and retry strategies |
| [05_templates/](05_templates/) | Ready-to-use ant templates |

---

## ğŸ¯ Ant Design Principles

### 1. Single Responsibility

Each ant does ONE thing well:

```python
# GOOD: Focused ant
class ProductAnt(BaseAnt):
    """Scrapes product details from a product page."""
    pass

# BAD: Multi-purpose ant
class EverythingAnt(BaseAnt):
    """Scrapes products, reviews, seller info, and recommendations."""
    pass
```

### 2. Configurable, Not Hardcoded

```python
# GOOD: Configurable
class ProductAnt(BaseAnt):
    def __init__(self, base_url: str, selectors: dict = None):
        self.base_url = base_url
        self.selectors = selectors or self.default_selectors

# BAD: Hardcoded
class ProductAnt(BaseAnt):
    base_url = "https://example.com"
    title_selector = "h1.product-title"
```

### 3. Graceful Degradation

```python
# GOOD: Handles missing data
def extract(self, html):
    return {
        'title': self.safe_extract('.title', html) or 'Unknown',
        'price': self.safe_extract('.price', html),  # None is OK
        'stock': self.safe_extract('.stock', html) or 'Unknown'
    }
```

### 4. Observable

```python
# GOOD: Logs and metrics
class ProductAnt(BaseAnt):
    def scrape(self, url):
        self.logger.info(f"Scraping: {url}")
        start = time.time()
        
        result = self._scrape(url)
        
        self.metrics.record('scrape_duration', time.time() - start)
        self.metrics.increment('pages_scraped')
        
        return result
```

---

## ğŸ—ï¸ Ant Hierarchy

```
BaseAnt (abstract)
â”‚
â”œâ”€â”€ SimpleAnt
â”‚   â””â”€â”€ Single page, requests-based
â”‚
â”œâ”€â”€ BrowserAnt
â”‚   â””â”€â”€ JavaScript rendering with Playwright
â”‚
â”œâ”€â”€ PaginatedAnt
â”‚   â””â”€â”€ Handles multi-page results
â”‚
â”œâ”€â”€ CrawlerAnt
â”‚   â””â”€â”€ Follows links, discovers pages
â”‚
â””â”€â”€ APIAnt
    â””â”€â”€ JSON API consumption
```

---

## ğŸ”„ Ant Lifecycle

```
1. INITIALIZE
   â””â”€â”€ Load config, setup session, validate inputs

2. FETCH
   â””â”€â”€ Make HTTP request or render with browser

3. VALIDATE
   â””â”€â”€ Check response status, detect blocks

4. PARSE
   â””â”€â”€ Convert raw response to parseable format

5. EXTRACT
   â””â”€â”€ Pull structured data using selectors

6. TRANSFORM
   â””â”€â”€ Clean, normalize, validate data

7. OUTPUT
   â””â”€â”€ Return structured result or save to storage

8. CLEANUP
   â””â”€â”€ Close connections, report metrics
```

---

## ğŸ’¡ Quick Start

### Minimal Ant

```python
from base_ant import BaseAnt

class MyAnt(BaseAnt):
    def extract(self, soup):
        return {
            'title': soup.select_one('h1').get_text(strip=True),
            'price': soup.select_one('.price').get_text(strip=True)
        }

# Usage
ant = MyAnt()
result = ant.scrape('https://example.com/product')
print(result)
```

### With Configuration

```python
ant = MyAnt(
    headers={'User-Agent': 'MyBot/1.0'},
    delay=2.0,
    retry_count=3,
    proxy='http://proxy:8080'
)
```

---

## ğŸ”— Related Sections

- [00_foundations/](../00_foundations/) - Core concepts and terminology
- [02_ant_farms/](../02_ant_farms/) - Collections of ants for specific sites
- [06_utils/](../06_utils/) - Shared utilities used by ants

---

*Part of the [Leaf Cutter Web Enriching Harvester Ants](../) framework*
